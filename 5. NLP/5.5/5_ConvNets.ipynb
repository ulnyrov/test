{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Классификация текстов сверточными нейронными сетями [convolutional neural network]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Слой свертки\n",
    "\n",
    "#### Фильтр [filter]:\n",
    "* $w_{1,n}$ – последовательность слов, $k$  – размер окна\n",
    "* $w_i$ , $d_{emb}$ – размерность эмбеддинга слова,  $\\textbf{w}_i \\in \\mathbb{R}^{d_{emb}} $\n",
    "* $\\textbf{x}_i = [\\textbf{w}_{i}, \\textbf{w}_{i+1}, \\ldots, \\textbf{w}_{i+k-1}]$, $\\textbf{x}_i \\in \\mathbb{R}^{k d_{emb}}$\n",
    "\n",
    "Фильтр: $p_i = g(\\textbf{x}_i  u)$, $p_i \\in \\mathbb{R}$, $u \\in \\mathbb{R}^{k d_{emb}}$\n",
    "\n",
    "\n",
    "![title](img/cnn1.png)\n",
    "\n",
    "\n",
    "Преобразуем каждое входное окно, но пока размерность входа не уменьшается!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Слой субдискретизации (пулинга, [pooling])\n",
    "\n",
    "* $h_i$ – выходные значения фильтра\n",
    "\n",
    "$\\max$-пулинг:\t$c = \\max_i h_i$\n",
    "\n",
    "\n",
    "![title](img/cnn2.png)\n",
    "\n",
    "* Выбираем самый важный признак из полученных на предыдущем шаге \n",
    "* Можем использовать и $\\min$, и усреднение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификатор на основе сверточной сети\n",
    "\n",
    "* $y \\in [0,1] $ - истинные значения\n",
    "* $\\widehat{y} = c$ - предсказанные значения\n",
    "\n",
    "![title](img/cnn3.png)\n",
    "\n",
    "Для обучения сверточной сети можно использовать обычный алгоритм распространения ошибки\n",
    "\n",
    "Одномерные фильтры – это сильное ограничение. Что делать, если $c=0.5$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Многомерные фильтры\n",
    "\n",
    "Используем $\\textit{l}$ разных фильтров: $u_{1}, \\ldots, u_{\\textit{l}}$: \n",
    "\n",
    "$\\textbf{x}_i = [\\textbf{w}_{i}, \\textbf{w}_{i+1}, \\ldots, \\textbf{w}_{i+k-1}]$\n",
    "\n",
    "$\\textbf{p}_i = g(\\textbf{x}_i \\cdot  U+b)$\n",
    "\n",
    "$\\textbf{p}_i \\in \\mathbb{R}^{\\textit{l}} $, $\\textbf{x}_i \\in \\mathbb{R}^{k d_{emb}}$, $U \\in \\mathbb{R}^{k d_{emb} \\times \\textit{l}}$, $b \\in \\mathbb{R}^{\\textit{l}} $\n",
    "\n",
    "![title](img/cnn4.png)\n",
    "\n",
    "\n",
    "$\\max$-пулинг:\t$c_j = \\max_i h_{i,j}, j \\in [0,\\textit{l}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Шаг окна \n",
    "Можно использовать непересекающиеся окна, чтобы уменьшить объем вычисления\n",
    "\n",
    "![title](img/cnn5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как выбирать вектора слов? \n",
    "* Случайная инициализация (если нет обученных моделей word2vec, GloVe)\n",
    "* word2vec, GloVe без обновления\n",
    "* word2vec, GloVe c обновлением на каждой эпохе (увеличивается количество параметров!)\n",
    "* Несколько каналов: копируем два входа и\n",
    "    * на один подаем word2vec и не обновляем эти входы во время обучения, на второй подаем word2vec и обновляем эти входы во время обучения\n",
    "    * на один вход подаем word2vec, на второй – GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/cnn6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/cnn7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.layers import Embedding, Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Masking\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "random.seed(1228)\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import  concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train examples 151978\n",
      "total test examples 74856\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "m = Mystem()\n",
    "\n",
    "\n",
    "regex = re.compile(\"[А-Яа-я:=!\\)\\()A-z\\_\\%/|]+\")\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    try:\n",
    "        return \" \".join(regex.findall(text))\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize(text, mystem=m):\n",
    "    try:\n",
    "        return \"\".join(m.lemmatize(text)).strip()  \n",
    "    except:\n",
    "        return \" \"\n",
    "\n",
    "\n",
    "df_neg = pd.read_csv(\"../data/negative.csv\", sep=';', header = None, usecols = [3])\n",
    "df_pos = pd.read_csv(\"../data/positive.csv\", sep=';', header = None, usecols = [3])\n",
    "df_neg['sent'] = 'neg'\n",
    "df_pos['sent'] = 'pos'\n",
    "df = pd.concat([df_neg, df_pos])\n",
    "df.columns = ['text', 'sent']\n",
    "df.text = df.text.apply(words_only)\n",
    "df.text = df.text.apply(lemmatize)\n",
    "\n",
    "\n",
    "X = df.text.tolist()\n",
    "y = df.sent.tolist()\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "X_text_train, X_text_test, y_train, y_test = train_test_split(X,y, test_size=0.33)\n",
    "print (\"total train examples %s\" % len(y_train))\n",
    "print (\"total test examples %s\" % len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "TEXT_LENGTH = 10\n",
    "VOCABULARY_SIZE = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "DIMS = 250\n",
    "MAX_FEATURES = 5000\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "filter_length = 3\n",
    "\n",
    "nb_filter = 50\n",
    "\n",
    "hidden_dims = 100\n",
    "nb_epoch = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x11cb85860>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_text_train)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_text_train)\n",
    "X_train = pad_sequences(sequences, maxlen=TEXT_LENGTH)\n",
    "sequences = tokenizer.texts_to_sequences(X_text_test)\n",
    "X_test = pad_sequences(sequences, maxlen=TEXT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0, 53041,   307,    93,    29,\n",
       "        9751], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(['pos', 'neg'])\n",
    "y_train_cat = np_utils.to_categorical(le.transform(y_train), 2)\n",
    "y_test_cat = np_utils.to_categorical(le.transform(y_test), 2)\n",
    "\n",
    "print(y_train_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 43s, sys: 8.99 s, total: 2min 52s\n",
      "Wall time: 2min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "emb_path = '/NLP/embeddings/wiki.ru.vec'\n",
    "\n",
    "words = []\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(emb_path)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    if len(values) == 301:\n",
    "        word = values[0]\n",
    "        words.append(word)\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1775997\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158491"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_input = Input(shape=(TEXT_LENGTH,), dtype='int32', name='words_input')\n",
    "\n",
    "\n",
    "\n",
    "#Our word embedding layer\n",
    "wordsEmbeddingLayer = Embedding(embedding_matrix.shape[0],\n",
    "                    embedding_matrix.shape[1],                                     \n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False)\n",
    "\n",
    "\n",
    "words = wordsEmbeddingLayer(words_input)\n",
    "\n",
    "\n",
    "words_conv = Convolution1D(filters=nb_filter,\n",
    "                            kernel_size=filter_length,\n",
    "                            padding='same',\n",
    "                            activation='relu',\n",
    "                            strides=1)(words)\n",
    "\n",
    "words_conv = GlobalMaxPooling1D()(words_conv)  \n",
    "\n",
    "output = words_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = Dropout(0.5)(output)\n",
    "output = Dense(hidden_dims, activation='tanh', kernel_regularizer=keras.regularizers.l2(0.01))(output)\n",
    "output = Dropout(0.25)(output)\n",
    "output = Dense(2, activation='softmax',  kernel_regularizer=keras.regularizers.l2(0.01))(output)\n",
    "\n",
    "model = Model(inputs=[words_input], outputs=[output])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train_cat, epochs=nb_epoch, batch_size=batch_size,  validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred.argmax(axis=-1)\n",
    "y_pred = le.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, y_pred, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, y_pred, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, y_pred, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "sns.heatmap(data=confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\", cbar=False, xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание\n",
    "\n",
    "Реализуйте сверточную сеть с несколькими каналами с фильтрами разной размерности (т.е. с разными входными свертками ```words_conv```). \n",
    "\n",
    "Размеры фильтров: ```filter_lengths = [1,2,3]```.\n",
    "\n",
    "Для конкатенации сверток используйте ```concatenate'''."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n'*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Решение\n",
    "\n",
    "words_input = Input(shape=(TEXT_LENGTH,), dtype='int32', name='words_input')\n",
    "\n",
    "wordsEmbeddingLayer = Embedding(embedding_matrix.shape[0],\n",
    "                    embedding_matrix.shape[1],                                     \n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False)\n",
    "\n",
    "words = wordsEmbeddingLayer(words_input)\n",
    "\n",
    "words_convolutions = []\n",
    "for filter_length in filter_lengths:\n",
    "    words_conv = Convolution1D(filters=nb_filter,\n",
    "                            kernel_size=filter_length,\n",
    "                            padding='same',\n",
    "                            activation='relu',\n",
    "                            strides=1)(words)\n",
    "                            \n",
    "    words_conv = GlobalMaxPooling1D()(words_conv)      \n",
    "    \n",
    "    words_convolutions.append(words_conv)  \n",
    "\n",
    "output = concatenate(words_convolutions)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
